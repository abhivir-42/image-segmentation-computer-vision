{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XRxHiKdGHiT"
      },
      "source": [
        "# Coursework 2: Image segmentation\n",
        "\n",
        "In this coursework you will develop and train a convolutional neural network for brain tumour image segmentation. Please read both the text and the code in this notebook to get an idea what you are expected to implement. Pay attention to the missing code blocks that look like this:\n",
        "\n",
        "```\n",
        "### Insert your code ###\n",
        "...\n",
        "### End of your code ###\n",
        "```\n",
        "\n",
        "## What to do?\n",
        "\n",
        "* Complete and run the code using `jupyter-lab` or `jupyter-notebook` to get the results.\n",
        "\n",
        "* Export (File | Save and Export Notebook As...) the notebook as a PDF file, which contains your code, results and answers, and upload the PDF file onto [Scientia](https://scientia.doc.ic.ac.uk).\n",
        "\n",
        "* Instead of clicking the Export button, you can also run the following command instead: `jupyter nbconvert coursework.ipynb --to pdf`\n",
        "\n",
        "* If Jupyter complains about some problems in exporting, it is likely that pandoc (https://pandoc.org/installing.html) or latex is not installed, or their paths have not been included. You can install the relevant libraries and retry.\n",
        "\n",
        "* If Jupyter-lab does not work for you at the end, you can use Google Colab to write the code and export the PDF file.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "You need to install Jupyter-Lab (https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) and other libraries used in this coursework, such as by running the command:\n",
        "`pip3 install [package_name]`\n",
        "\n",
        "## GPU resource\n",
        "\n",
        "The coursework is developed to be able to run on CPU, as all images have been pre-processed to be 2D and of a smaller size, compared to original 3D volumes.\n",
        "\n",
        "However, to save training time, you may want to use GPU. In that case, you can run this notebook on Google Colab. On Google Colab, go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware acceleartor. At the end, please still export everything and submit as a PDF file on Scientia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Eq1KWmR3HWYV"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "# These libraries should be sufficient for this tutorial.\n",
        "# However, if any other library is needed, please install by yourself.\n",
        "import tarfile\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4TX-CXBHW4c"
      },
      "source": [
        "## 1. Download and visualise the imaging dataset.\n",
        "\n",
        "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from T1-Gd contrast enhanced 3D brain volumes and downsample the images.\n",
        "\n",
        "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
        "\n",
        "- 0: background\n",
        "- 1: edema\n",
        "- 2: non-enhancing tumour\n",
        "- 3: enhancing tumour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mt93oQ8xZkE9",
        "outputId": "51fc16ab-2d35-46c1-883b-d619841d8b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-21 13:14:00--  https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/4bf8fqcfgf3lebiv2in99/Task01_BrainTumour_2D.tar.gz?rlkey=ceq898g2tr3aaxjxn4xjxbob1 [following]\n",
            "--2025-02-21 13:14:00--  https://www.dropbox.com/scl/fi/4bf8fqcfgf3lebiv2in99/Task01_BrainTumour_2D.tar.gz?rlkey=ceq898g2tr3aaxjxn4xjxbob1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc618eed96f73659472f555818d7.dl.dropboxusercontent.com/cd/0/inline/Ckg71Rw7U9qSeGQCUUFqqGMVx8b-NzG_IZCQxy-8i-sH_LOKFfdvfwrPvSmLEUsvhLJ9fobQ4sa-ufG0ER-Rz4PtPQ0Vb2IkDhk5aTUzSRmYQeTcsv8uOxx6Q48096ZulQ0/file# [following]\n",
            "--2025-02-21 13:14:00--  https://uc618eed96f73659472f555818d7.dl.dropboxusercontent.com/cd/0/inline/Ckg71Rw7U9qSeGQCUUFqqGMVx8b-NzG_IZCQxy-8i-sH_LOKFfdvfwrPvSmLEUsvhLJ9fobQ4sa-ufG0ER-Rz4PtPQ0Vb2IkDhk5aTUzSRmYQeTcsv8uOxx6Q48096ZulQ0/file\n",
            "Resolving uc618eed96f73659472f555818d7.dl.dropboxusercontent.com (uc618eed96f73659472f555818d7.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to uc618eed96f73659472f555818d7.dl.dropboxusercontent.com (uc618eed96f73659472f555818d7.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Ckh9mntE751meMS-DzDKR7dxIuQE9Uk5RKyvY4VCSkUjD-iD172YuD_IebOA1Ze2FtdNjUrX67yqx6i9rJLirFI3Jbh-GUJWZ8YmLzKQXs7od56UcyHWaoZzeb8dsMvuRfirHO0IBECKdfKCzBp0_bN8MTaFb5a4K8QytxBAYlyZfRp8utPEeMhz1mQHoMXDs_WQojHG3D2uVJC63z53UR-79Q2S6h5UCn58oyowaRG8EiG2i3bYSpQCFysQu-sqgfoUzCyZO3sm7Q4aQ44vOzh9jbTNaJ24Hmd_J3yJS_cg_P5D6ftGX55kqFnAlSsyNjQ_qUlxwp_Bqy8PtoXdQCjoAWspOvJGs3P-9gqC2pdPBA/file [following]\n",
            "--2025-02-21 13:14:01--  https://uc618eed96f73659472f555818d7.dl.dropboxusercontent.com/cd/0/inline2/Ckh9mntE751meMS-DzDKR7dxIuQE9Uk5RKyvY4VCSkUjD-iD172YuD_IebOA1Ze2FtdNjUrX67yqx6i9rJLirFI3Jbh-GUJWZ8YmLzKQXs7od56UcyHWaoZzeb8dsMvuRfirHO0IBECKdfKCzBp0_bN8MTaFb5a4K8QytxBAYlyZfRp8utPEeMhz1mQHoMXDs_WQojHG3D2uVJC63z53UR-79Q2S6h5UCn58oyowaRG8EiG2i3bYSpQCFysQu-sqgfoUzCyZO3sm7Q4aQ44vOzh9jbTNaJ24Hmd_J3yJS_cg_P5D6ftGX55kqFnAlSsyNjQ_qUlxwp_Bqy8PtoXdQCjoAWspOvJGs3P-9gqC2pdPBA/file\n",
            "Reusing existing connection to uc618eed96f73659472f555818d7.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9251149 (8.8M) [application/octet-stream]\n",
            "Saving to: ‘Task01_BrainTumour_2D.tar.gz’\n",
            "\n",
            "Task01_BrainTumour_ 100%[===================>]   8.82M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-21 13:14:01 (75.7 MB/s) - ‘Task01_BrainTumour_2D.tar.gz’ saved [9251149/9251149]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
        "\n",
        "# Unzip the '.tar.gz' file to the current directory\n",
        "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
        "datafile.extractall()\n",
        "datafile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu_BTL0x6o5a"
      },
      "source": [
        "## Visualise a random set of 4 training images along with their label maps.\n",
        "\n",
        "Suggested colour map for brain MR image:\n",
        "```\n",
        "cmap = 'gray'\n",
        "```\n",
        "\n",
        "Suggested colour map for segmentation map:\n",
        "```\n",
        "cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fgubCRC6m4k"
      },
      "outputs": [],
      "source": [
        "### Insert your code ###\n",
        "\n",
        "# Get paths to training images and labels\n",
        "image_dir = 'Task01_BrainTumour_2D/training_images'\n",
        "label_dir = 'Task01_BrainTumour_2D/training_labels'\n",
        "\n",
        "# Get list of image files\n",
        "image_files = sorted(os.listdir(image_dir))\n",
        "\n",
        "# Select 4 random indices\n",
        "random_indices = random.sample(range(len(image_files)), 4)\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(4, 2, figsize=(10, 20))\n",
        "fig.suptitle('Training Images and Labels', fontsize=16)\n",
        "\n",
        "# Plot each image and its label\n",
        "for i, idx in enumerate(random_indices):\n",
        "    # Read image and label\n",
        "    image_path = os.path.join(image_dir, image_files[idx])\n",
        "    label_path = os.path.join(label_dir, image_files[idx])\n",
        "\n",
        "    image = imageio.imread(image_path)\n",
        "    label = imageio.imread(label_path)\n",
        "\n",
        "    # Plot image\n",
        "    axes[i, 0].imshow(image, cmap='gray')\n",
        "    axes[i, 0].set_title(f'Image {idx}')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Plot label\n",
        "    cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
        "    axes[i, 1].imshow(label, cmap=cmap)\n",
        "    axes[i, 1].set_title(f'Label {idx}')\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### End of your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xWGT3KaML-D"
      },
      "source": [
        "## 2. Implement a dataset class.\n",
        "\n",
        "It can read the imaging dataset and get items, pairs of images and label maps, as training batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p6wFZ3na5z9"
      },
      "outputs": [],
      "source": [
        "def normalise_intensity(image, thres_roi=1.0):\n",
        "    \"\"\" Normalise the image intensity by the mean and standard deviation \"\"\"\n",
        "    # ROI defines the image foreground\n",
        "    val_l = np.percentile(image, thres_roi)\n",
        "    roi = (image >= val_l)\n",
        "    mu, sigma = np.mean(image[roi]), np.std(image[roi])\n",
        "    eps = 1e-6\n",
        "    image2 = (image - mu) / (sigma + eps)\n",
        "    return image2\n",
        "\n",
        "\n",
        "class BrainImageSet(Dataset):\n",
        "    \"\"\" Brain image set \"\"\"\n",
        "    def __init__(self, image_path, label_path='', deploy=False):\n",
        "        self.image_path = image_path\n",
        "        self.deploy = deploy\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        image_names = sorted(os.listdir(image_path))\n",
        "        for image_name in image_names:\n",
        "            # Read the image\n",
        "            image = imageio.imread(os.path.join(image_path, image_name))\n",
        "            self.images += [image]\n",
        "\n",
        "            # Read the label map\n",
        "            if not self.deploy:\n",
        "                label_name = os.path.join(label_path, image_name)\n",
        "                label = imageio.imread(label_name)\n",
        "                self.labels += [label]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get an image and perform intensity normalisation\n",
        "        # Dimension: XY\n",
        "        image = normalise_intensity(self.images[idx])\n",
        "\n",
        "        # Get its label map\n",
        "        # Dimension: XY\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def get_random_batch(self, batch_size):\n",
        "        # Get a batch of paired images and label maps\n",
        "        # Dimension of images: NCXY\n",
        "        # Dimension of labels: NXY\n",
        "        images, labels = [], []\n",
        "\n",
        "        ### Insert your code ###\n",
        "        ...\n",
        "        ### End of your code ###\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa4ZpawDNmwu"
      },
      "source": [
        "## 3. Build a U-net architecture.\n",
        "\n",
        "You will implement a U-net architecture. If you are not familiar with U-net, please read this paper:\n",
        "\n",
        "[1] Olaf Ronneberger et al. [U-Net: Convolutional networks for biomedical image segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). MICCAI, 2015.\n",
        "\n",
        "For the first convolutional layer, you can start with 16 filters. We have implemented the encoder path. Please complete the decoder path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMPmBZVGb1aI"
      },
      "outputs": [],
      "source": [
        "\"\"\" U-net \"\"\"\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channel=1, output_channel=1, num_filter=16):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # BatchNorm: by default during training this layer keeps running estimates\n",
        "        # of its computed mean and variance, which are then used for normalization\n",
        "        # during evaluation.\n",
        "\n",
        "        # Encoder path\n",
        "        n = num_filter  # 16\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 32\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 64\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        n *= 2  # 128\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(n),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder path\n",
        "        ### Insert your code ###\n",
        "        ...\n",
        "        ### End of your code ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use the convolutional operators defined above to build the U-net\n",
        "        # The encoder part is already done for you.\n",
        "        # You need to complete the decoder part.\n",
        "        # Encoder\n",
        "        x = self.conv1(x)\n",
        "        conv1_skip = x\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        conv2_skip = x\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        conv3_skip = x\n",
        "\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # Decoder\n",
        "        ### Insert your code ###\n",
        "        ...\n",
        "        ### End of your code ###\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcNWZS08d47P"
      },
      "source": [
        "## 4. Train the segmentation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaGGkKQndIaR"
      },
      "outputs": [],
      "source": [
        "# CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device: {0}'.format(device))\n",
        "\n",
        "# Build the model\n",
        "num_class = 4\n",
        "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
        "model = model.to(device)\n",
        "params = list(model.parameters())\n",
        "\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(params, lr=1e-3)\n",
        "\n",
        "# Segmentation loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Datasets\n",
        "train_set = BrainImageSet('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\n",
        "test_set = BrainImageSet('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n",
        "\n",
        "# Train the model\n",
        "# Note: when you debug the model, you may reduce the number of iterations or batch size to save time.\n",
        "num_iter = 10000\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 16\n",
        "start = time.time()\n",
        "for it in range(1, 1 + num_iter):\n",
        "    # Set the modules in training mode, which will have effects on certain modules, e.g. dropout or batchnorm.\n",
        "    start_iter = time.time()\n",
        "    model.train()\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = train_set.get_random_batch(train_batch_size)\n",
        "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
        "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
        "    logits = model(images)\n",
        "\n",
        "    # Perform optimisation and print out the training loss\n",
        "    ### Insert your code ###\n",
        "    ...\n",
        "    ### End of your code ###\n",
        "\n",
        "    # Evaluate\n",
        "    if it % 100 == 0:\n",
        "        model.eval()\n",
        "        # Disabling gradient calculation during reference to reduce memory consumption\n",
        "        with torch.no_grad():\n",
        "            # Evaluate on a batch of test images and print out the test loss\n",
        "            ### Insert your code ###\n",
        "            ...\n",
        "            ### End of your code ###\n",
        "\n",
        "    # Save the model\n",
        "    if it % 5000 == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, 'model_{0}.pt'.format(it)))\n",
        "print('Training took {:.3f}s in total.'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89yjxjGyb6yT"
      },
      "source": [
        "## 5. Deploy the trained model to a random set of 4 test images and visualise the automated segmentation.\n",
        "\n",
        "You can show the images as a 4 x 3 panel. Each row shows one example, with the 3 columns being the test image, automated segmentation and ground truth segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZeLE0qZjd2j"
      },
      "outputs": [],
      "source": [
        "### Insert your code ###\n",
        "...\n",
        "### End of your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj3Qusin_s_r"
      },
      "source": [
        "## 6. Discussion. Does your trained model work well? How would you improve this model so it can be deployed to the real clinic?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVwEtDKIdTRs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}